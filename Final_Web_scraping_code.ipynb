{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yQ4T3GwPBT_S"
      },
      "outputs": [],
      "source": [
        "### Imports for wedscraping\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from IPython.core.formatters import catch_format_error\n",
        "import regex as re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### urls for the three rightmove locations to scrape\n",
        "b_site = 'https://www.rightmove.co.uk'\n",
        "## Newcastle\n",
        "#url = 'https://www.rightmove.co.uk/property-for-sale/find.html?locationIdentifier=REGION%5E984&radius=3.0&sortType=10&index=0&propertyTypes=detached%2Csemi-detached%2Cterraced&includeSSTC=false&mustHave=&dontShow=&furnishTypes=&keywords='\n",
        "## Middlesbrough\n",
        "#url = https://www.rightmove.co.uk/property-for-sale/find.html?locationIdentifier=REGION%5E933&radius=5.0&propertyTypes=&includeSSTC=false&mustHave=&dontShow=&furnishTypes=&keywords=\n",
        "## Sunderland\n",
        "url = 'https://www.rightmove.co.uk/property-for-sale/find.html?locationIdentifier=REGION%5E1295&radius=3.0&index=0&sortType=10&propertyTypes=&includeSSTC=false&mustHave=&dontShow=&furnishTypes=&keywords='"
      ],
      "metadata": {
        "id": "vnFSighEBZyM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Testing the website response\n",
        "response = requests.get(url)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJvWIGYHB7_U",
        "outputId": "b91eb330-efe6-4b1b-a8a6-c0eefe1ea1cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start of scraping"
      ],
      "metadata": {
        "id": "nT4t2IadCL2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###Creating a list of numbers for the index to navigate between all of the pages on rightmove\n",
        "\n",
        "numbers = range(0,984,24)\n",
        "\n",
        "index_num = []\n",
        "for num in numbers:\n",
        "  index_num.append(num)"
      ],
      "metadata": {
        "id": "IuhGMcD9CAZv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Getting the links to all properties on all pages\n",
        "\n",
        "# Initiating an empty list\n",
        "links = []\n",
        "\n",
        "# Using the index_num list to loop through each of the 42 pages on the site\n",
        "for i in index_num:\n",
        "  site = f'https://www.rightmove.co.uk/property-for-sale/find.html?locationIdentifier=REGION%5E1295&radius=3.0&index={i}&sortType=10&propertyTypes=&includeSSTC=false&mustHave=&dontShow=&furnishTypes=&keywords='\n",
        " \n",
        "\n",
        "  resp = requests.get(site)\n",
        "  soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "\n",
        "  # Looping through each page link to retrieve each property link\n",
        "  for link in soup.find_all('div', class_  ='propertyCard-details'):\n",
        "    try:\n",
        "      # Appending the links list with each property link\n",
        "      links.append(link.a.get('href'))\n",
        "    except Exception as e:\n",
        "      print('')\n",
        "\n",
        "  for link in links:\n",
        "    if link == '/properties/58852044#/?channel=RES_BUY':\n",
        "      continue\n",
        "    else:\n",
        "      # Creating the full hyperlink from the scraped one using the base link (2nd cell)\n",
        "      homes =  [b_site + x for x in links]"
      ],
      "metadata": {
        "id": "R24bBVUqCTC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### loop to colect all data from the property pages (price, bedrooms, house type and address)\n",
        "\n",
        "prices = []\n",
        "address = []\n",
        "htype = []\n",
        "bedrooms = []\n",
        "\n",
        "for link in homes:\n",
        "\n",
        "    # The error handling for each link append is for the properties that haven't been \n",
        "    # typed intot he site correctly. These links wil be handled after.\n",
        "    resp = requests.get(link)\n",
        "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "\n",
        "    try:\n",
        "      prices.append(soup.find('div',class_  = '_1gfnqJ3Vtd1z40MlC0MzXu').text)\n",
        "    except Exception as e:\n",
        "      print(link)\n",
        "    \n",
        "    try:\n",
        "      address.append(soup.find('div', class_ = 'OojFk4MTxFDKIfqreGNt0').text)\n",
        "    except Exception as e:\n",
        "      print(link)\n",
        "    \n",
        "    try:\n",
        "      htype.append(soup.find_all('div',class_  = '_3OGW_s5TH6aUqi4uHum5Gy')[0].text)\n",
        "    except Exception as e:\n",
        "      print(link)\n",
        "\n",
        "    try:\n",
        "      bedrooms.append(soup.find_all('div',class_  = '_3OGW_s5TH6aUqi4uHum5Gy')[1].text)\n",
        "    except Exception as e:\n",
        "      print(link)"
      ],
      "metadata": {
        "id": "8ltbmpkTDIzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Creating a dictionary with each of the lists scraped\n",
        "dict = {'price':prices, 'address':address, 'house_type':htype, 'bedroom':bedrooms}"
      ],
      "metadata": {
        "id": "KnubzNyvCWmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Creating a dataframe with the dictionary\n",
        "df = pd.DataFrame(dict)"
      ],
      "metadata": {
        "id": "up6BbUszDfH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Saving the scraped df as a csv\n",
        "df.to_csv('scraped_data.csv')"
      ],
      "metadata": {
        "id": "U9yC_aCUDwpc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}